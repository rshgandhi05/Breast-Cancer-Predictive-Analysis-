{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Breast Cancer Survival Period and Recommending Treatments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team:**\n",
    "* Fady Smouni\n",
    "* Rishu Gandhi\n",
    "\n",
    "**Course:** DAAN 888 – Design and Implementation of Analytics System (Fall, 2021).                   \n",
    "**Supervisor:** Robin Qiu, Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Objectives\n",
    "* We aim to perform classification based on various parameters to determine a survival range for the patient.\n",
    "* We would also like to try and perform a regression analysis to see If it is possible to predict how long a patient will survive in months.\n",
    "* Try to explore the use of Bayesian Networks in the understanding which treatment(s) could most likely prolong breast cancer patients’ survival time at their different survival periods based on a survival causal analysis.\n",
    "    \n",
    "\n",
    "### Project Importance and Impacts\n",
    "Research shows that breast cancer can affect 1 in 8 U.S. women. That is why it is important to do everything we can to add value to the immense research that is done to fight this disease. Significant progress has been achieved over the past decades, bringing survival rates higher and allowing patients to live longer. Through our research, we aim to use deep neural networks to help drive this progress further by providing a clearer picture of patient survival and potentially start a conversation about life extension in relation to the various available treatments.\n",
    "\n",
    "**Keywords:** Breast Cancer, Survival Chance, Patient, Deep Neural Network\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "* Data Access Source(url): https://seer.cancer.gov/data/access.html\n",
    "\n",
    "* Short Description : The data was collected from Surveillance, Epidemiology, and End Results (SEER) program trough the National Institute of Health (NIH). It comprises information about the patient such as age and race. It also provides an extensive list of medical parameters of which tumor information, surgery information, and vital status. The original text dataset that is available for download has 840666 records.\n",
    "* This dataset plays a major role in our goal to predict Breast Cancer survivability using various medical and demographical parameters. We are able to use this labeled data to train our model to discern between individuals that survived more 6 years or less and individuals that survived more than 6 years.\n",
    "\n",
    "\n",
    "**Keywords:** Breast Cancer, NIH, SEER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problem\n",
    "\n",
    "* Deep Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages\n",
    "\n",
    "* Please make sure you install any of the following packages that you are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code begins here\n",
    "\n",
    "# Remove the \"#\" for the packages you are missing\n",
    "#!pip install matplotlib\n",
    "#!pip install tensorflow\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install sklearn\n",
    "#!pip install causalnex --user\n",
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm\n",
    "import causalnex\n",
    "physical_devices = tf.config.list_physical_devices('GPU')                  # This is a necessary configuration to avoid\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True) # any memory errors when running the RNN model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from causalnex.structure.notears import from_pandas\n",
    "from causalnex.structure import StructureModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from causalnex.inference import InferenceEngine\n",
    "from causalnex.network import BayesianNetwork\n",
    "from causalnex.evaluation import roc_auc\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by reading the extracted csv file that has all the variables deemed necessary to answer our question.\n",
    "cancer_data = pd.read_csv('breast_cancer_completeV1.csv')\n",
    "\n",
    "# We drop any rows with no values\n",
    "cancer_data.dropna(inplace=True)\n",
    "\n",
    "#removing male gender because negligeable representation\n",
    "cancer_data.drop(cancer_data[cancer_data['SEX'] == 1].index, inplace = True)\n",
    "#removing sex column because only 1 value there now\n",
    "cancer_data = cancer_data.drop(['SEX'], axis='columns')\n",
    "\n",
    "#reformate no_surg column to boolean or surgery (0) or no surgery (1)\n",
    "cancer_data[\"NO_SURG\"].replace({1: 1, 2: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}, inplace=True)\n",
    "\n",
    "#drop rows with unkown age =99 and drop under 30 \n",
    "cancer_data.drop(cancer_data[cancer_data['AGE_1REC'] < 7].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['AGE_1REC'] == 99].index, inplace = True)\n",
    "\n",
    "#combine other group for rac_reca and add unkown to other\n",
    "cancer_data[\"RAC_RECA\"].replace({7: 2, 9: 2, 3: 2}, inplace=True)\n",
    "\n",
    "#combine marital stauts to has a partner or single\n",
    "cancer_data.drop(cancer_data[cancer_data['MAR_STAT'] == 9].index, inplace = True)\n",
    "cancer_data[\"MAR_STAT\"].replace({3: 1, 4: 1, 5: 1, 6: 2}, inplace=True)\n",
    "\n",
    "#get rid of any rows that are not alive or dead because of breast cancer\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50060].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50300].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50130].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 27030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50080].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50160].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50050].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 37000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 22030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 27040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50210].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50051].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50120].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21100].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 35021].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50100].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50150].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21050].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 31010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 29020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21130].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 23000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50070].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 29010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 27010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 38000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 32020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 34000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21080].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 41000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 33040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 22020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21072].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 29030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50220].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 35011].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50200].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50180].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 33010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 30000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 25010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 20050].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50140].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21071].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50170].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 24000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 35043].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 27070].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 27050].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 27060].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 35012].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21110].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 35031].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 35023].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 25020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 20070].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 35013].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 29040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 20030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 20080].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 28010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 20040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50190].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21120].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50230].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21090].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 27020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21060].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 21010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 20060].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 35022].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 20100].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 32010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 22010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50110].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 20020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 35041].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 50090].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUB'] == 22060].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50060].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50300].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50130].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 27030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50080].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50160].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50050].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 37000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 22030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 27040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50210].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50051].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50120].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21100].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 35021].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50100].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50150].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21050].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 31010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 29020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21130].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 23000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50070].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 29010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 27010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 38000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 32020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 34000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21080].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 41000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 33040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 22020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21072].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 29030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50220].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 35011].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50200].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50180].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 33010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 30000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 25010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 20050].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50140].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21071].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50170].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 24000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 35043].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 27070].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 27050].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 27060].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 35012].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21110].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 35031].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 35023].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 25020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 20070].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 35013].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 29040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 20030].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 20080].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 28010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 20040].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50190].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21120].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50230].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21090].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 27020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21060].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 21010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 20060].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 35022].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 20100].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 32010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 22010].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50110].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50000].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 20020].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 35041].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 50090].index, inplace = True)\n",
    "cancer_data.drop(cancer_data[cancer_data['CODPUBKM'] == 22060].index, inplace = True)\n",
    "cancer_data = cancer_data.drop(['CODPUB'], axis='columns')\n",
    "cancer_data = cancer_data.drop(['CODPUBKM'], axis='columns')\n",
    "\n",
    "#drop unkown survival time\n",
    "cancer_data.drop(cancer_data[cancer_data['SRV_TIME_MON'] == 9999].index, inplace = True)\n",
    "\n",
    "#group insured and insured/no specifics\n",
    "cancer_data[\"INSREC_PUB\"].replace({4: 3}, inplace=True)\n",
    "\n",
    "#get rid of rows with unkown stage in ADJAJCCSTG\n",
    "cancer_data.drop(cancer_data[cancer_data['ADJAJCCSTG'] == 99].index, inplace = True)\n",
    "\n",
    "#get rid of rows with unkown tumor counts MALIGCOUNT\n",
    "cancer_data.drop(cancer_data[cancer_data['MALIGCOUNT'] == 99].index, inplace = True)\n",
    "\n",
    "#set the conditions for the binary split and perform the split\n",
    "conditions = [(cancer_data['SRV_TIME_MON'] < 73), (cancer_data['SRV_TIME_MON'] > 72)]\n",
    "choices = ['0', '1']\n",
    "cancer_data['Survival_category'] = np.select(conditions, choices, default=\"N/A\")\n",
    "\n",
    "#drop the survival time in months as it is no longer relevant\n",
    "cancer_data = cancer_data.drop(['SRV_TIME_MON'], axis='columns')\n",
    "\n",
    "#displaying part of the dataframe if further investigation is necessary by uncommenting the line of code below\n",
    "#cancer_data\n",
    "\n",
    "#create dummy variables for the categorical variables\n",
    "dummies = pd.get_dummies(cancer_data, columns = ['MAR_STAT', 'SEQ_NUM', 'PRIMSITE', 'LATERAL', 'GRADE', 'SURGPRIF', 'SURGSITF', \n",
    "                                                'NO_SURG', \n",
    "                                                'AGE_1REC', 'BEHTREND', 'RAC_RECA', 'STAT_REC', 'ERSTATUS',\n",
    "                                                'PRSTATUS', 'INSREC_PUB', 'ADJTM_6VALUE', 'ADJNM_6VALUE', 'ADJM_6VALUE', \n",
    "                                                'ADJAJCCSTG', 'her2', 'brst_sub', 'MALIGCOUNT', 'BENBORDCOUNT',\n",
    "                                                'RADIATION','RAD_SURG_SEQ','CHEMO'])\n",
    "\n",
    "#final dataframe\n",
    "cancer_data_final = dummies\n",
    "\n",
    "# Initialize data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "#split into labels and data\n",
    "data = cancer_data_final.drop(['Survival_category'], axis='columns')\n",
    "labels= cancer_data_final[\"Survival_category\"]\n",
    "\n",
    "#create an array of both the data and labels\n",
    "#no scalling is necessary for any of the variables as we are using binary representation for all of them\n",
    "#data_array\n",
    "data_array = np.array(data)\n",
    "#labels_array\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "#reshape the data_array to meet the input requirement for the RNN model\n",
    "data_array_resh = data_array.reshape(208517, 197, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for models\n",
    "# We first need to split our dataset into training, validation, and testing\n",
    "# 80% for training, 20% for testing, and 10% of the training data for validation\n",
    "(trainX, testX, trainY, testY) = train_test_split(data_array_resh, labels_array, test_size=0.20, random_state=73)\n",
    "(trainX, validationX, trainY, validationY) = train_test_split(trainX, trainY, test_size=0.10, random_state=55)\n",
    "\n",
    "# We use LabelBinarizer to convert the labels into vectors so that we can use them in our models\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.fit_transform(testY)\n",
    "validationY = lb.fit_transform(validationY)\n",
    "\n",
    "#We also set up our inputshape to only have to change it once if that is necessary in the future.\n",
    "input_shape = (197, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1669/1669 [==============================] - 42s 23ms/step - loss: 0.6448 - accuracy: 0.6582\n",
      "Epoch 2/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.6423 - accuracy: 0.6590\n",
      "Epoch 3/100\n",
      "1669/1669 [==============================] - 41s 24ms/step - loss: 0.6424 - accuracy: 0.6590\n",
      "Epoch 4/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.6423 - accuracy: 0.6590\n",
      "Epoch 5/100\n",
      "1669/1669 [==============================] - 54s 32ms/step - loss: 0.6422 - accuracy: 0.6590\n",
      "Epoch 6/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.6423 - accuracy: 0.6590\n",
      "Epoch 7/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.5076 - accuracy: 0.7680\n",
      "Epoch 8/100\n",
      "1669/1669 [==============================] - 41s 25ms/step - loss: 0.3602 - accuracy: 0.8824\n",
      "Epoch 9/100\n",
      "1669/1669 [==============================] - 45s 27ms/step - loss: 0.3573 - accuracy: 0.8839\n",
      "Epoch 10/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3567 - accuracy: 0.8836\n",
      "Epoch 11/100\n",
      "1669/1669 [==============================] - 42s 25ms/step - loss: 0.3541 - accuracy: 0.8851\n",
      "Epoch 12/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3530 - accuracy: 0.8855\n",
      "Epoch 13/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3523 - accuracy: 0.8853\n",
      "Epoch 14/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3520 - accuracy: 0.8854\n",
      "Epoch 15/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3516 - accuracy: 0.8851\n",
      "Epoch 16/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3505 - accuracy: 0.8855\n",
      "Epoch 17/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3507 - accuracy: 0.8855\n",
      "Epoch 18/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3495 - accuracy: 0.8856\n",
      "Epoch 19/100\n",
      "1669/1669 [==============================] - 41s 24ms/step - loss: 0.3485 - accuracy: 0.8858\n",
      "Epoch 20/100\n",
      "1669/1669 [==============================] - 42s 25ms/step - loss: 0.3484 - accuracy: 0.8857\n",
      "Epoch 21/100\n",
      "1669/1669 [==============================] - 45s 27ms/step - loss: 0.3478 - accuracy: 0.8859\n",
      "Epoch 22/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3466 - accuracy: 0.8862\n",
      "Epoch 23/100\n",
      "1669/1669 [==============================] - 43s 26ms/step - loss: 0.3447 - accuracy: 0.8868\n",
      "Epoch 24/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3443 - accuracy: 0.8869\n",
      "Epoch 25/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3442 - accuracy: 0.8868\n",
      "Epoch 26/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3442 - accuracy: 0.8871\n",
      "Epoch 27/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3439 - accuracy: 0.8869\n",
      "Epoch 28/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3440 - accuracy: 0.8867\n",
      "Epoch 29/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3436 - accuracy: 0.8871\n",
      "Epoch 30/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3453 - accuracy: 0.88600s - loss: 0\n",
      "Epoch 31/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3422 - accuracy: 0.8876\n",
      "Epoch 32/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3420 - accuracy: 0.8873\n",
      "Epoch 33/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3418 - accuracy: 0.8874\n",
      "Epoch 34/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3419 - accuracy: 0.8872\n",
      "Epoch 35/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3420 - accuracy: 0.8876\n",
      "Epoch 36/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3410 - accuracy: 0.8877\n",
      "Epoch 37/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3409 - accuracy: 0.8875\n",
      "Epoch 38/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3405 - accuracy: 0.8878\n",
      "Epoch 39/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3400 - accuracy: 0.8879\n",
      "Epoch 40/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3476 - accuracy: 0.8843\n",
      "Epoch 41/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3433 - accuracy: 0.8870\n",
      "Epoch 42/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3420 - accuracy: 0.8870\n",
      "Epoch 43/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3406 - accuracy: 0.88750s - los\n",
      "Epoch 44/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3409 - accuracy: 0.88710s - loss: 0.3410 - \n",
      "Epoch 45/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3386 - accuracy: 0.8884\n",
      "Epoch 46/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3374 - accuracy: 0.8886\n",
      "Epoch 47/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3363 - accuracy: 0.8887\n",
      "Epoch 48/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3353 - accuracy: 0.8897\n",
      "Epoch 49/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3340 - accuracy: 0.8900\n",
      "Epoch 50/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3324 - accuracy: 0.8907\n",
      "Epoch 51/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3297 - accuracy: 0.8917\n",
      "Epoch 52/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3293 - accuracy: 0.89130s - los\n",
      "Epoch 53/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3260 - accuracy: 0.8929\n",
      "Epoch 54/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3241 - accuracy: 0.8939\n",
      "Epoch 55/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3217 - accuracy: 0.8950\n",
      "Epoch 56/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3210 - accuracy: 0.8955\n",
      "Epoch 57/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3169 - accuracy: 0.8974\n",
      "Epoch 58/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3175 - accuracy: 0.8978\n",
      "Epoch 59/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3149 - accuracy: 0.8985\n",
      "Epoch 60/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3132 - accuracy: 0.8996\n",
      "Epoch 61/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3127 - accuracy: 0.8997\n",
      "Epoch 62/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3111 - accuracy: 0.9005\n",
      "Epoch 63/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3150 - accuracy: 0.89880s - loss: 0.3150 \n",
      "Epoch 64/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3130 - accuracy: 0.8994\n",
      "Epoch 65/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3117 - accuracy: 0.9005\n",
      "Epoch 66/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3104 - accuracy: 0.9010\n",
      "Epoch 67/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3098 - accuracy: 0.9014\n",
      "Epoch 68/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3093 - accuracy: 0.9015\n",
      "Epoch 69/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3093 - accuracy: 0.9014\n",
      "Epoch 70/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3086 - accuracy: 0.9016\n",
      "Epoch 71/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3086 - accuracy: 0.90170s - loss: 0.3\n",
      "Epoch 72/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3116 - accuracy: 0.9001\n",
      "Epoch 73/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3079 - accuracy: 0.9019\n",
      "Epoch 74/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3075 - accuracy: 0.9022\n",
      "Epoch 75/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3099 - accuracy: 0.9011\n",
      "Epoch 76/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3077 - accuracy: 0.9021\n",
      "Epoch 77/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3067 - accuracy: 0.9023\n",
      "Epoch 78/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3074 - accuracy: 0.9022\n",
      "Epoch 79/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3077 - accuracy: 0.9021\n",
      "Epoch 80/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3073 - accuracy: 0.9020\n",
      "Epoch 81/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3064 - accuracy: 0.9026\n",
      "Epoch 82/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3073 - accuracy: 0.9021\n",
      "Epoch 83/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3065 - accuracy: 0.9025\n",
      "Epoch 84/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3062 - accuracy: 0.9028\n",
      "Epoch 85/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3065 - accuracy: 0.9024\n",
      "Epoch 86/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3063 - accuracy: 0.9026\n",
      "Epoch 87/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3059 - accuracy: 0.9027\n",
      "Epoch 88/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3059 - accuracy: 0.9027\n",
      "Epoch 89/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3059 - accuracy: 0.9026\n",
      "Epoch 90/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3056 - accuracy: 0.90270s - loss: 0.3056 - accu\n",
      "Epoch 91/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3056 - accuracy: 0.9029\n",
      "Epoch 92/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3051 - accuracy: 0.9029\n",
      "Epoch 93/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3053 - accuracy: 0.9028\n",
      "Epoch 94/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3050 - accuracy: 0.9030\n",
      "Epoch 95/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3054 - accuracy: 0.9028\n",
      "Epoch 96/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3052 - accuracy: 0.9030\n",
      "Epoch 97/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3044 - accuracy: 0.9030\n",
      "Epoch 98/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3047 - accuracy: 0.9031\n",
      "Epoch 99/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3045 - accuracy: 0.9030\n",
      "Epoch 100/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3048 - accuracy: 0.9029\n",
      "accuracy: 90.26%\n",
      "Epoch 1/100\n",
      "1669/1669 [==============================] - 39s 22ms/step - loss: 0.6445 - accuracy: 0.6590\n",
      "Epoch 2/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.6426 - accuracy: 0.6590\n",
      "Epoch 3/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.5781 - accuracy: 0.7138\n",
      "Epoch 4/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3866 - accuracy: 0.8720\n",
      "Epoch 5/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3722 - accuracy: 0.8762\n",
      "Epoch 6/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3674 - accuracy: 0.8785\n",
      "Epoch 7/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3635 - accuracy: 0.8792\n",
      "Epoch 8/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3605 - accuracy: 0.8802\n",
      "Epoch 9/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3574 - accuracy: 0.8818\n",
      "Epoch 10/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3546 - accuracy: 0.8840\n",
      "Epoch 11/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3534 - accuracy: 0.8843\n",
      "Epoch 12/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3532 - accuracy: 0.8844\n",
      "Epoch 13/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3518 - accuracy: 0.8848\n",
      "Epoch 14/100\n",
      "1669/1669 [==============================] - 44s 27ms/step - loss: 0.3515 - accuracy: 0.8848\n",
      "Epoch 15/100\n",
      "1669/1669 [==============================] - 41s 24ms/step - loss: 0.3501 - accuracy: 0.8854\n",
      "Epoch 16/100\n",
      "1669/1669 [==============================] - 43s 26ms/step - loss: 0.3493 - accuracy: 0.8857\n",
      "Epoch 17/100\n",
      "1669/1669 [==============================] - 43s 26ms/step - loss: 0.3488 - accuracy: 0.8859\n",
      "Epoch 18/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3479 - accuracy: 0.8862\n",
      "Epoch 19/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3473 - accuracy: 0.8863\n",
      "Epoch 20/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3466 - accuracy: 0.8865\n",
      "Epoch 21/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3456 - accuracy: 0.8860\n",
      "Epoch 22/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3452 - accuracy: 0.8865\n",
      "Epoch 23/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3443 - accuracy: 0.8871\n",
      "Epoch 24/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3440 - accuracy: 0.8869\n",
      "Epoch 25/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3438 - accuracy: 0.88681s - loss: 0.3436 - accuracy: 0.88 - ETA: 0s - los\n",
      "Epoch 26/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3431 - accuracy: 0.8870\n",
      "Epoch 27/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3428 - accuracy: 0.8873\n",
      "Epoch 28/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3424 - accuracy: 0.8874\n",
      "Epoch 29/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3420 - accuracy: 0.8873\n",
      "Epoch 30/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3408 - accuracy: 0.8878\n",
      "Epoch 31/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3406 - accuracy: 0.8882\n",
      "Epoch 32/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3407 - accuracy: 0.8879\n",
      "Epoch 33/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3391 - accuracy: 0.8883\n",
      "Epoch 34/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3381 - accuracy: 0.8891\n",
      "Epoch 35/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3368 - accuracy: 0.88921s - loss: 0 - ETA: 1s\n",
      "Epoch 36/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3348 - accuracy: 0.8895\n",
      "Epoch 37/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3306 - accuracy: 0.8914\n",
      "Epoch 38/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3268 - accuracy: 0.8930\n",
      "Epoch 39/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3210 - accuracy: 0.8960\n",
      "Epoch 40/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3185 - accuracy: 0.8973\n",
      "Epoch 41/100\n",
      "1669/1669 [==============================] - 41s 24ms/step - loss: 0.3243 - accuracy: 0.8954\n",
      "Epoch 42/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3164 - accuracy: 0.8986\n",
      "Epoch 43/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3149 - accuracy: 0.8992\n",
      "Epoch 44/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3128 - accuracy: 0.9003\n",
      "Epoch 45/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3123 - accuracy: 0.9007\n",
      "Epoch 46/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3123 - accuracy: 0.9005\n",
      "Epoch 47/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3117 - accuracy: 0.9011\n",
      "Epoch 48/100\n",
      "1669/1669 [==============================] - 41s 24ms/step - loss: 0.3111 - accuracy: 0.9009\n",
      "Epoch 49/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3104 - accuracy: 0.9016\n",
      "Epoch 50/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3105 - accuracy: 0.9013\n",
      "Epoch 51/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3096 - accuracy: 0.9019\n",
      "Epoch 52/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3092 - accuracy: 0.90200s - loss: 0.3094 - accuracy: 0.\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3089 - accuracy: 0.9021\n",
      "Epoch 54/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3090 - accuracy: 0.9022\n",
      "Epoch 55/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3091 - accuracy: 0.9021\n",
      "Epoch 56/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3088 - accuracy: 0.9020\n",
      "Epoch 57/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3082 - accuracy: 0.9023\n",
      "Epoch 58/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3086 - accuracy: 0.9021\n",
      "Epoch 59/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3086 - accuracy: 0.9020\n",
      "Epoch 60/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3076 - accuracy: 0.9023\n",
      "Epoch 61/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3080 - accuracy: 0.90222s - los\n",
      "Epoch 62/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3072 - accuracy: 0.9023\n",
      "Epoch 63/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3079 - accuracy: 0.9024\n",
      "Epoch 64/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3071 - accuracy: 0.9025\n",
      "Epoch 65/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3070 - accuracy: 0.9025\n",
      "Epoch 66/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3065 - accuracy: 0.9026\n",
      "Epoch 67/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3065 - accuracy: 0.9026\n",
      "Epoch 68/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3067 - accuracy: 0.9025\n",
      "Epoch 69/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3060 - accuracy: 0.9028\n",
      "Epoch 70/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3064 - accuracy: 0.9025\n",
      "Epoch 71/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3066 - accuracy: 0.9026\n",
      "Epoch 72/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3063 - accuracy: 0.9027\n",
      "Epoch 73/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3061 - accuracy: 0.9028\n",
      "Epoch 74/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3062 - accuracy: 0.9028\n",
      "Epoch 75/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3061 - accuracy: 0.9026\n",
      "Epoch 76/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3055 - accuracy: 0.9029\n",
      "Epoch 77/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3057 - accuracy: 0.9028\n",
      "Epoch 78/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3063 - accuracy: 0.9027\n",
      "Epoch 79/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3048 - accuracy: 0.9030\n",
      "Epoch 80/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3054 - accuracy: 0.9030\n",
      "Epoch 81/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3052 - accuracy: 0.9030\n",
      "Epoch 82/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3055 - accuracy: 0.9028\n",
      "Epoch 83/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3052 - accuracy: 0.9030\n",
      "Epoch 84/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3051 - accuracy: 0.9030\n",
      "Epoch 85/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3052 - accuracy: 0.9031\n",
      "Epoch 86/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3054 - accuracy: 0.9031\n",
      "Epoch 87/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3052 - accuracy: 0.9031\n",
      "Epoch 88/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3049 - accuracy: 0.9030\n",
      "Epoch 89/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3047 - accuracy: 0.9031\n",
      "Epoch 90/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3046 - accuracy: 0.9032\n",
      "Epoch 91/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3049 - accuracy: 0.9029\n",
      "Epoch 92/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3051 - accuracy: 0.9031\n",
      "Epoch 93/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3043 - accuracy: 0.9031\n",
      "Epoch 94/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3043 - accuracy: 0.9032\n",
      "Epoch 95/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3050 - accuracy: 0.9029\n",
      "Epoch 96/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3044 - accuracy: 0.9031\n",
      "Epoch 97/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3046 - accuracy: 0.9031\n",
      "Epoch 98/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3044 - accuracy: 0.90310s - loss: 0.3044 - ac\n",
      "Epoch 99/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3046 - accuracy: 0.9031\n",
      "Epoch 100/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3042 - accuracy: 0.9031\n",
      "accuracy: 90.33%\n",
      "Epoch 1/100\n",
      "1669/1669 [==============================] - 40s 23ms/step - loss: 0.6453 - accuracy: 0.6585\n",
      "Epoch 2/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.6425 - accuracy: 0.6590\n",
      "Epoch 3/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.6423 - accuracy: 0.6590\n",
      "Epoch 4/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.5901 - accuracy: 0.6995\n",
      "Epoch 5/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3598 - accuracy: 0.8806\n",
      "Epoch 6/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3565 - accuracy: 0.8830\n",
      "Epoch 7/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3551 - accuracy: 0.8838\n",
      "Epoch 8/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3556 - accuracy: 0.8831\n",
      "Epoch 9/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3535 - accuracy: 0.8843\n",
      "Epoch 10/100\n",
      "1669/1669 [==============================] - 37s 22ms/step - loss: 0.3521 - accuracy: 0.8845\n",
      "Epoch 11/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.4228 - accuracy: 0.8366\n",
      "Epoch 12/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3681 - accuracy: 0.8754\n",
      "Epoch 13/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3668 - accuracy: 0.8756\n",
      "Epoch 14/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3512 - accuracy: 0.8844\n",
      "Epoch 15/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3499 - accuracy: 0.8851\n",
      "Epoch 16/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3493 - accuracy: 0.8850\n",
      "Epoch 17/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3501 - accuracy: 0.8850\n",
      "Epoch 18/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3481 - accuracy: 0.8858\n",
      "Epoch 19/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3480 - accuracy: 0.8858\n",
      "Epoch 20/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3483 - accuracy: 0.8853\n",
      "Epoch 21/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3500 - accuracy: 0.8842\n",
      "Epoch 22/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3458 - accuracy: 0.8864\n",
      "Epoch 23/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3475 - accuracy: 0.8858\n",
      "Epoch 24/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3460 - accuracy: 0.8860\n",
      "Epoch 25/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3460 - accuracy: 0.8862\n",
      "Epoch 26/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3504 - accuracy: 0.8843\n",
      "Epoch 27/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3457 - accuracy: 0.8866\n",
      "Epoch 28/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3448 - accuracy: 0.8863\n",
      "Epoch 29/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3450 - accuracy: 0.8863\n",
      "Epoch 30/100\n",
      "1669/1669 [==============================] - 37s 22ms/step - loss: 0.3442 - accuracy: 0.8863\n",
      "Epoch 31/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3442 - accuracy: 0.8866\n",
      "Epoch 32/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3435 - accuracy: 0.8867\n",
      "Epoch 33/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3454 - accuracy: 0.8860\n",
      "Epoch 34/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3429 - accuracy: 0.8870\n",
      "Epoch 35/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3432 - accuracy: 0.8867\n",
      "Epoch 36/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3434 - accuracy: 0.8866\n",
      "Epoch 37/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3423 - accuracy: 0.8871\n",
      "Epoch 38/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3425 - accuracy: 0.8874\n",
      "Epoch 39/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3425 - accuracy: 0.8871\n",
      "Epoch 40/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3420 - accuracy: 0.8871\n",
      "Epoch 41/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3423 - accuracy: 0.8873\n",
      "Epoch 42/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3420 - accuracy: 0.8873\n",
      "Epoch 43/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3416 - accuracy: 0.8874\n",
      "Epoch 44/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3420 - accuracy: 0.8873\n",
      "Epoch 45/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3417 - accuracy: 0.8872\n",
      "Epoch 46/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3415 - accuracy: 0.8876\n",
      "Epoch 47/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3411 - accuracy: 0.8876\n",
      "Epoch 48/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3409 - accuracy: 0.8877\n",
      "Epoch 49/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3409 - accuracy: 0.8875\n",
      "Epoch 50/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3405 - accuracy: 0.8877\n",
      "Epoch 51/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3402 - accuracy: 0.8879\n",
      "Epoch 52/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3404 - accuracy: 0.8875\n",
      "Epoch 53/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3399 - accuracy: 0.8880\n",
      "Epoch 54/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3403 - accuracy: 0.8878\n",
      "Epoch 55/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3400 - accuracy: 0.8878\n",
      "Epoch 56/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3400 - accuracy: 0.8880\n",
      "Epoch 57/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3394 - accuracy: 0.8880\n",
      "Epoch 58/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3393 - accuracy: 0.8880\n",
      "Epoch 59/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3391 - accuracy: 0.8882\n",
      "Epoch 60/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3391 - accuracy: 0.8881\n",
      "Epoch 61/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3386 - accuracy: 0.8880\n",
      "Epoch 62/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3381 - accuracy: 0.8884\n",
      "Epoch 63/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3384 - accuracy: 0.8883\n",
      "Epoch 64/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3379 - accuracy: 0.8886\n",
      "Epoch 65/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3380 - accuracy: 0.8887\n",
      "Epoch 66/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3372 - accuracy: 0.8885\n",
      "Epoch 67/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3365 - accuracy: 0.8888\n",
      "Epoch 68/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3353 - accuracy: 0.88910s - loss: 0.3354 - accura\n",
      "Epoch 69/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3338 - accuracy: 0.8896\n",
      "Epoch 70/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3392 - accuracy: 0.8867\n",
      "Epoch 71/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3381 - accuracy: 0.8879\n",
      "Epoch 72/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3347 - accuracy: 0.8888\n",
      "Epoch 73/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3321 - accuracy: 0.8898\n",
      "Epoch 74/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3273 - accuracy: 0.8918\n",
      "Epoch 75/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3225 - accuracy: 0.8941\n",
      "Epoch 76/100\n",
      "1669/1669 [==============================] - 38s 22ms/step - loss: 0.3188 - accuracy: 0.8961\n",
      "Epoch 77/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3155 - accuracy: 0.89760s - loss: 0.3155 - accuracy: 0.\n",
      "Epoch 78/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3140 - accuracy: 0.8989\n",
      "Epoch 79/100\n",
      "1669/1669 [==============================] - 37s 22ms/step - loss: 0.3125 - accuracy: 0.8997\n",
      "Epoch 80/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3119 - accuracy: 0.8998\n",
      "Epoch 81/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3102 - accuracy: 0.9005\n",
      "Epoch 82/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3092 - accuracy: 0.9009\n",
      "Epoch 83/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3090 - accuracy: 0.9010\n",
      "Epoch 84/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3092 - accuracy: 0.9011\n",
      "Epoch 85/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3084 - accuracy: 0.9012\n",
      "Epoch 86/100\n",
      "1669/1669 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.90 - 38s 23ms/step - loss: 0.3082 - accuracy: 0.9011\n",
      "Epoch 87/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3078 - accuracy: 0.9014\n",
      "Epoch 88/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3074 - accuracy: 0.9016\n",
      "Epoch 89/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3077 - accuracy: 0.9016\n",
      "Epoch 90/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3073 - accuracy: 0.9017\n",
      "Epoch 91/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3073 - accuracy: 0.90170s - loss: 0\n",
      "Epoch 92/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3070 - accuracy: 0.9016\n",
      "Epoch 93/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3067 - accuracy: 0.9018\n",
      "Epoch 94/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3064 - accuracy: 0.9018\n",
      "Epoch 95/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3066 - accuracy: 0.9019\n",
      "Epoch 96/100\n",
      "1669/1669 [==============================] - 37s 22ms/step - loss: 0.3066 - accuracy: 0.9018\n",
      "Epoch 97/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3062 - accuracy: 0.9018\n",
      "Epoch 98/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3062 - accuracy: 0.9019\n",
      "Epoch 99/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3060 - accuracy: 0.90200s - loss: 0.3061 - accuracy: 0.\n",
      "Epoch 100/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3061 - accuracy: 0.9021\n",
      "accuracy: 90.50%\n",
      "Epoch 1/100\n",
      "1669/1669 [==============================] - 39s 22ms/step - loss: 0.6447 - accuracy: 0.6590\n",
      "Epoch 2/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.6425 - accuracy: 0.6590\n",
      "Epoch 3/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.5627 - accuracy: 0.7240\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3859 - accuracy: 0.8642\n",
      "Epoch 5/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3638 - accuracy: 0.87810s - loss: 0.3638 - \n",
      "Epoch 6/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3597 - accuracy: 0.8807\n",
      "Epoch 7/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3567 - accuracy: 0.8824\n",
      "Epoch 8/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3545 - accuracy: 0.8842\n",
      "Epoch 9/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3532 - accuracy: 0.8852TA: 0s - loss: 0.3532 - accuracy: 0.\n",
      "Epoch 10/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3517 - accuracy: 0.8858\n",
      "Epoch 11/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3509 - accuracy: 0.88610s - los\n",
      "Epoch 12/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3499 - accuracy: 0.8864\n",
      "Epoch 13/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3499 - accuracy: 0.8863\n",
      "Epoch 14/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3495 - accuracy: 0.8864\n",
      "Epoch 15/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3491 - accuracy: 0.8868\n",
      "Epoch 16/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3487 - accuracy: 0.8869\n",
      "Epoch 17/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3485 - accuracy: 0.8869\n",
      "Epoch 18/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3478 - accuracy: 0.8870\n",
      "Epoch 19/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3474 - accuracy: 0.8872\n",
      "Epoch 20/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3471 - accuracy: 0.8872\n",
      "Epoch 21/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3465 - accuracy: 0.8871\n",
      "Epoch 22/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3459 - accuracy: 0.8872\n",
      "Epoch 23/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3455 - accuracy: 0.8872\n",
      "Epoch 24/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3446 - accuracy: 0.8875\n",
      "Epoch 25/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3445 - accuracy: 0.8873\n",
      "Epoch 26/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3443 - accuracy: 0.8875\n",
      "Epoch 27/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3443 - accuracy: 0.8873\n",
      "Epoch 28/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3435 - accuracy: 0.8874\n",
      "Epoch 29/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3431 - accuracy: 0.8879\n",
      "Epoch 30/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3425 - accuracy: 0.8877\n",
      "Epoch 31/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3423 - accuracy: 0.8881\n",
      "Epoch 32/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3417 - accuracy: 0.88790s - loss: 0.3415 - \n",
      "Epoch 33/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3417 - accuracy: 0.8882\n",
      "Epoch 34/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3413 - accuracy: 0.8880\n",
      "Epoch 35/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3404 - accuracy: 0.8884\n",
      "Epoch 36/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3400 - accuracy: 0.8886\n",
      "Epoch 37/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3390 - accuracy: 0.8888\n",
      "Epoch 38/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3386 - accuracy: 0.8889\n",
      "Epoch 39/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3372 - accuracy: 0.8891\n",
      "Epoch 40/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3360 - accuracy: 0.8895\n",
      "Epoch 41/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3334 - accuracy: 0.8901\n",
      "Epoch 42/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3294 - accuracy: 0.8915\n",
      "Epoch 43/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3257 - accuracy: 0.8935\n",
      "Epoch 44/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3218 - accuracy: 0.8950\n",
      "Epoch 45/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3196 - accuracy: 0.8966\n",
      "Epoch 46/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3182 - accuracy: 0.8972\n",
      "Epoch 47/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3162 - accuracy: 0.8977\n",
      "Epoch 48/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3148 - accuracy: 0.8985\n",
      "Epoch 49/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3137 - accuracy: 0.8997\n",
      "Epoch 50/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3126 - accuracy: 0.8998\n",
      "Epoch 51/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3113 - accuracy: 0.9006\n",
      "Epoch 52/100\n",
      "1669/1669 [==============================] - 43s 26ms/step - loss: 0.3108 - accuracy: 0.9008\n",
      "Epoch 53/100\n",
      "1669/1669 [==============================] - 43s 26ms/step - loss: 0.3097 - accuracy: 0.9014\n",
      "Epoch 54/100\n",
      "1669/1669 [==============================] - 43s 26ms/step - loss: 0.3094 - accuracy: 0.9017\n",
      "Epoch 55/100\n",
      "1669/1669 [==============================] - 46s 28ms/step - loss: 0.3089 - accuracy: 0.9019\n",
      "Epoch 56/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3085 - accuracy: 0.9018\n",
      "Epoch 57/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3077 - accuracy: 0.9021\n",
      "Epoch 58/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3074 - accuracy: 0.9024\n",
      "Epoch 59/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3071 - accuracy: 0.9024\n",
      "Epoch 60/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3068 - accuracy: 0.9024\n",
      "Epoch 61/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3063 - accuracy: 0.9027\n",
      "Epoch 62/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3064 - accuracy: 0.9025\n",
      "Epoch 63/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3061 - accuracy: 0.9026\n",
      "Epoch 64/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3055 - accuracy: 0.9028\n",
      "Epoch 65/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3055 - accuracy: 0.9027\n",
      "Epoch 66/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3054 - accuracy: 0.9029\n",
      "Epoch 67/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3053 - accuracy: 0.9029\n",
      "Epoch 68/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3046 - accuracy: 0.9029\n",
      "Epoch 69/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3049 - accuracy: 0.9029\n",
      "Epoch 70/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3047 - accuracy: 0.9029\n",
      "Epoch 71/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3047 - accuracy: 0.9030\n",
      "Epoch 72/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3047 - accuracy: 0.9029\n",
      "Epoch 73/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3044 - accuracy: 0.9030\n",
      "Epoch 74/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3046 - accuracy: 0.9030\n",
      "Epoch 75/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3043 - accuracy: 0.9030\n",
      "Epoch 76/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3041 - accuracy: 0.9030\n",
      "Epoch 77/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3043 - accuracy: 0.90310s - loss: 0\n",
      "Epoch 78/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3039 - accuracy: 0.9030\n",
      "Epoch 79/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3038 - accuracy: 0.9029\n",
      "Epoch 80/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3038 - accuracy: 0.9032\n",
      "Epoch 81/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3041 - accuracy: 0.9031\n",
      "Epoch 82/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3037 - accuracy: 0.9030\n",
      "Epoch 83/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3033 - accuracy: 0.9031\n",
      "Epoch 84/100\n",
      "1669/1669 [==============================] - ETA: 0s - loss: 0.3031 - accuracy: 0.90 - 39s 23ms/step - loss: 0.3031 - accuracy: 0.9031\n",
      "Epoch 85/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3034 - accuracy: 0.9031\n",
      "Epoch 86/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3033 - accuracy: 0.9032\n",
      "Epoch 87/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3030 - accuracy: 0.9032\n",
      "Epoch 88/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3031 - accuracy: 0.9032\n",
      "Epoch 89/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3030 - accuracy: 0.9032\n",
      "Epoch 90/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3030 - accuracy: 0.9031\n",
      "Epoch 91/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3028 - accuracy: 0.9032\n",
      "Epoch 92/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3029 - accuracy: 0.9032\n",
      "Epoch 93/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3025 - accuracy: 0.9033\n",
      "Epoch 94/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3026 - accuracy: 0.9032\n",
      "Epoch 95/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3025 - accuracy: 0.9033\n",
      "Epoch 96/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3024 - accuracy: 0.9033\n",
      "Epoch 97/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3019 - accuracy: 0.9033\n",
      "Epoch 98/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3022 - accuracy: 0.9033\n",
      "Epoch 99/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3022 - accuracy: 0.9033\n",
      "Epoch 100/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3022 - accuracy: 0.9033\n",
      "accuracy: 90.30%\n",
      "Epoch 1/100\n",
      "1669/1669 [==============================] - 40s 23ms/step - loss: 0.6450 - accuracy: 0.6581\n",
      "Epoch 2/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.6427 - accuracy: 0.6590\n",
      "Epoch 3/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.6423 - accuracy: 0.6590\n",
      "Epoch 4/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.4424 - accuracy: 0.8184\n",
      "Epoch 5/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3600 - accuracy: 0.8805\n",
      "Epoch 6/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3547 - accuracy: 0.8838\n",
      "Epoch 7/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3526 - accuracy: 0.8846\n",
      "Epoch 8/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3514 - accuracy: 0.8852\n",
      "Epoch 9/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3505 - accuracy: 0.8854\n",
      "Epoch 10/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3492 - accuracy: 0.8857\n",
      "Epoch 11/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3487 - accuracy: 0.8862\n",
      "Epoch 12/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3478 - accuracy: 0.8863\n",
      "Epoch 13/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3468 - accuracy: 0.8868\n",
      "Epoch 14/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3471 - accuracy: 0.8865\n",
      "Epoch 15/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3473 - accuracy: 0.8867\n",
      "Epoch 16/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3464 - accuracy: 0.8868\n",
      "Epoch 17/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3461 - accuracy: 0.8871\n",
      "Epoch 18/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3460 - accuracy: 0.88730s - loss: 0.3463 - \n",
      "Epoch 19/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3454 - accuracy: 0.8876\n",
      "Epoch 20/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3453 - accuracy: 0.88750s - loss: 0.3453 - accuracy: 0.88\n",
      "Epoch 21/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3449 - accuracy: 0.8874\n",
      "Epoch 22/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3449 - accuracy: 0.8876\n",
      "Epoch 23/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3453 - accuracy: 0.8876\n",
      "Epoch 24/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3452 - accuracy: 0.8876\n",
      "Epoch 25/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3448 - accuracy: 0.8876\n",
      "Epoch 26/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3443 - accuracy: 0.8879\n",
      "Epoch 27/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3442 - accuracy: 0.8878\n",
      "Epoch 28/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3444 - accuracy: 0.8879\n",
      "Epoch 29/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3440 - accuracy: 0.8877\n",
      "Epoch 30/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3500 - accuracy: 0.88350s - loss: 0\n",
      "Epoch 31/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3465 - accuracy: 0.8858\n",
      "Epoch 32/100\n",
      "1669/1669 [==============================] - 38s 22ms/step - loss: 0.3443 - accuracy: 0.8875\n",
      "Epoch 33/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3448 - accuracy: 0.8873\n",
      "Epoch 34/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3446 - accuracy: 0.8874\n",
      "Epoch 35/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3448 - accuracy: 0.8874\n",
      "Epoch 36/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3447 - accuracy: 0.88720s - loss: 0.3448 \n",
      "Epoch 37/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3439 - accuracy: 0.8876\n",
      "Epoch 38/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3445 - accuracy: 0.8874\n",
      "Epoch 39/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3442 - accuracy: 0.8876\n",
      "Epoch 40/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3443 - accuracy: 0.88730s - loss: 0.3443 - ac\n",
      "Epoch 41/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3442 - accuracy: 0.8876\n",
      "Epoch 42/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3437 - accuracy: 0.8875\n",
      "Epoch 43/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3437 - accuracy: 0.8876\n",
      "Epoch 44/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3445 - accuracy: 0.8875\n",
      "Epoch 45/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3435 - accuracy: 0.8877\n",
      "Epoch 46/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3433 - accuracy: 0.8877\n",
      "Epoch 47/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3432 - accuracy: 0.8877\n",
      "Epoch 48/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3430 - accuracy: 0.88770s - loss: 0.3433 - \n",
      "Epoch 49/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3431 - accuracy: 0.8880\n",
      "Epoch 50/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3436 - accuracy: 0.8876\n",
      "Epoch 51/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3428 - accuracy: 0.8881\n",
      "Epoch 52/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3427 - accuracy: 0.8879\n",
      "Epoch 53/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3428 - accuracy: 0.8878\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3426 - accuracy: 0.8879\n",
      "Epoch 55/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3467 - accuracy: 0.8852\n",
      "Epoch 56/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3420 - accuracy: 0.8881\n",
      "Epoch 57/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3418 - accuracy: 0.8881\n",
      "Epoch 58/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3423 - accuracy: 0.8881\n",
      "Epoch 59/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3416 - accuracy: 0.8881\n",
      "Epoch 60/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3419 - accuracy: 0.8882\n",
      "Epoch 61/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3410 - accuracy: 0.8886\n",
      "Epoch 62/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3410 - accuracy: 0.8883\n",
      "Epoch 63/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3413 - accuracy: 0.8883\n",
      "Epoch 64/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3411 - accuracy: 0.8885\n",
      "Epoch 65/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3410 - accuracy: 0.8889\n",
      "Epoch 66/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3408 - accuracy: 0.8888\n",
      "Epoch 67/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3422 - accuracy: 0.8876\n",
      "Epoch 68/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3403 - accuracy: 0.8887\n",
      "Epoch 69/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3404 - accuracy: 0.8888\n",
      "Epoch 70/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3398 - accuracy: 0.8889\n",
      "Epoch 71/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3396 - accuracy: 0.8890\n",
      "Epoch 72/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3396 - accuracy: 0.8887\n",
      "Epoch 73/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3390 - accuracy: 0.8892\n",
      "Epoch 74/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3391 - accuracy: 0.8893\n",
      "Epoch 75/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3384 - accuracy: 0.8894\n",
      "Epoch 76/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3381 - accuracy: 0.8895\n",
      "Epoch 77/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3382 - accuracy: 0.8894\n",
      "Epoch 78/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3358 - accuracy: 0.8899\n",
      "Epoch 79/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3338 - accuracy: 0.8907\n",
      "Epoch 80/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3302 - accuracy: 0.8915\n",
      "Epoch 81/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3255 - accuracy: 0.8937\n",
      "Epoch 82/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3316 - accuracy: 0.8912\n",
      "Epoch 83/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3212 - accuracy: 0.8958\n",
      "Epoch 84/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3168 - accuracy: 0.8986\n",
      "Epoch 85/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3152 - accuracy: 0.8992\n",
      "Epoch 86/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3099 - accuracy: 0.9016\n",
      "Epoch 87/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3087 - accuracy: 0.90190s - los\n",
      "Epoch 88/100\n",
      "1669/1669 [==============================] - 39s 24ms/step - loss: 0.3086 - accuracy: 0.9019\n",
      "Epoch 89/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3080 - accuracy: 0.9021\n",
      "Epoch 90/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3080 - accuracy: 0.90220s - los\n",
      "Epoch 91/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3073 - accuracy: 0.9025\n",
      "Epoch 92/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3071 - accuracy: 0.9025\n",
      "Epoch 93/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3067 - accuracy: 0.9027\n",
      "Epoch 94/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3063 - accuracy: 0.9027\n",
      "Epoch 95/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3072 - accuracy: 0.9027\n",
      "Epoch 96/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3062 - accuracy: 0.9029\n",
      "Epoch 97/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3061 - accuracy: 0.9028\n",
      "Epoch 98/100\n",
      "1669/1669 [==============================] - 40s 24ms/step - loss: 0.3054 - accuracy: 0.9031\n",
      "Epoch 99/100\n",
      "1669/1669 [==============================] - 38s 23ms/step - loss: 0.3055 - accuracy: 0.9031\n",
      "Epoch 100/100\n",
      "1669/1669 [==============================] - 39s 23ms/step - loss: 0.3055 - accuracy: 0.9031\n",
      "accuracy: 90.07%\n",
      "90.29% (+/- 0.14%)\n"
     ]
    }
   ],
   "source": [
    "# We run a 5-fold cross validation to make sure our model performs well over the entire dataset. This also allows us to later\n",
    "#train it on the entire dataset.\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "cvscores = []\n",
    "for train, test in kfold.split(data_array_resh, labels_array):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = 64, return_sequences = True, input_shape = input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(units = 32, input_shape = input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    epochs = 100\n",
    "    bs = 100\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(data_array_resh[train], lb.fit_transform(labels_array[train]), epochs = epochs, batch_size = bs)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(data_array_resh[test], lb.fit_transform(labels_array[test]), verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1502/1502 [==============================] - 38s 24ms/step - loss: 0.6451 - accuracy: 0.6581 - val_loss: 0.6428 - val_accuracy: 0.6577\n",
      "Epoch 2/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.6426 - accuracy: 0.6587 - val_loss: 0.6424 - val_accuracy: 0.6577\n",
      "Epoch 3/100\n",
      "1502/1502 [==============================] - 40s 26ms/step - loss: 0.4604 - accuracy: 0.8037 - val_loss: 0.3617 - val_accuracy: 0.8797\n",
      "Epoch 4/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3766 - accuracy: 0.8683 - val_loss: 0.3537 - val_accuracy: 0.8808\n",
      "Epoch 5/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3654 - accuracy: 0.8767 - val_loss: 0.3440 - val_accuracy: 0.8871\n",
      "Epoch 6/100\n",
      "1502/1502 [==============================] - 41s 27ms/step - loss: 0.3598 - accuracy: 0.8801 - val_loss: 0.3438 - val_accuracy: 0.8886\n",
      "Epoch 7/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3563 - accuracy: 0.8825 - val_loss: 0.3421 - val_accuracy: 0.8880\n",
      "Epoch 8/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3541 - accuracy: 0.8838 - val_loss: 0.3472 - val_accuracy: 0.8873\n",
      "Epoch 9/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3527 - accuracy: 0.8846 - val_loss: 0.3411 - val_accuracy: 0.8895\n",
      "Epoch 10/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3520 - accuracy: 0.8853 - val_loss: 0.3421 - val_accuracy: 0.8893\n",
      "Epoch 11/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3506 - accuracy: 0.8857 - val_loss: 0.3402 - val_accuracy: 0.8899\n",
      "Epoch 12/100\n",
      "1502/1502 [==============================] - 38s 26ms/step - loss: 0.3504 - accuracy: 0.8859 - val_loss: 0.3420 - val_accuracy: 0.8879\n",
      "Epoch 13/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3505 - accuracy: 0.8855 - val_loss: 0.3407 - val_accuracy: 0.8899\n",
      "Epoch 14/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3490 - accuracy: 0.8863 - val_loss: 0.3432 - val_accuracy: 0.8881\n",
      "Epoch 15/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3493 - accuracy: 0.8859 - val_loss: 0.3401 - val_accuracy: 0.8889\n",
      "Epoch 16/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3487 - accuracy: 0.8863 - val_loss: 0.3388 - val_accuracy: 0.8896\n",
      "Epoch 17/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3483 - accuracy: 0.8864 - val_loss: 0.3385 - val_accuracy: 0.8897\n",
      "Epoch 18/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3478 - accuracy: 0.8865 - val_loss: 0.3403 - val_accuracy: 0.8890\n",
      "Epoch 19/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3471 - accuracy: 0.8866 - val_loss: 0.3395 - val_accuracy: 0.8901\n",
      "Epoch 20/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3469 - accuracy: 0.8868 - val_loss: 0.3399 - val_accuracy: 0.8897\n",
      "Epoch 21/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3467 - accuracy: 0.8867 - val_loss: 0.3367 - val_accuracy: 0.8902\n",
      "Epoch 22/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3462 - accuracy: 0.8867 - val_loss: 0.3374 - val_accuracy: 0.8892\n",
      "Epoch 23/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3458 - accuracy: 0.8868 - val_loss: 0.3404 - val_accuracy: 0.8874\n",
      "Epoch 24/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3455 - accuracy: 0.8870 - val_loss: 0.3424 - val_accuracy: 0.8889\n",
      "Epoch 25/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3452 - accuracy: 0.8870 - val_loss: 0.3395 - val_accuracy: 0.8893\n",
      "Epoch 26/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3454 - accuracy: 0.8873 - val_loss: 0.3380 - val_accuracy: 0.8886\n",
      "Epoch 27/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3446 - accuracy: 0.8873 - val_loss: 0.3364 - val_accuracy: 0.8902\n",
      "Epoch 28/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3438 - accuracy: 0.8878 - val_loss: 0.3361 - val_accuracy: 0.8891\n",
      "Epoch 29/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3441 - accuracy: 0.8874 - val_loss: 0.3354 - val_accuracy: 0.8904\n",
      "Epoch 30/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3441 - accuracy: 0.8874 - val_loss: 0.3352 - val_accuracy: 0.8898\n",
      "Epoch 31/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3432 - accuracy: 0.8877 - val_loss: 0.3344 - val_accuracy: 0.8902\n",
      "Epoch 32/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3430 - accuracy: 0.8873 - val_loss: 0.3362 - val_accuracy: 0.8894\n",
      "Epoch 33/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3423 - accuracy: 0.8877 - val_loss: 0.3322 - val_accuracy: 0.8910\n",
      "Epoch 34/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3418 - accuracy: 0.8876 - val_loss: 0.3365 - val_accuracy: 0.8895\n",
      "Epoch 35/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3406 - accuracy: 0.8881 - val_loss: 0.3332 - val_accuracy: 0.8908\n",
      "Epoch 36/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3398 - accuracy: 0.8882 - val_loss: 0.3357 - val_accuracy: 0.8891\n",
      "Epoch 37/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3393 - accuracy: 0.8884 - val_loss: 0.3361 - val_accuracy: 0.8901\n",
      "Epoch 38/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3379 - accuracy: 0.8888 - val_loss: 0.3285 - val_accuracy: 0.8907\n",
      "Epoch 39/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3373 - accuracy: 0.8892 - val_loss: 0.3289 - val_accuracy: 0.8918\n",
      "Epoch 40/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3355 - accuracy: 0.8894 - val_loss: 0.3248 - val_accuracy: 0.8924\n",
      "Epoch 41/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3330 - accuracy: 0.8907 - val_loss: 0.3261 - val_accuracy: 0.8934\n",
      "Epoch 42/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3314 - accuracy: 0.8909 - val_loss: 0.3180 - val_accuracy: 0.8949\n",
      "Epoch 43/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3283 - accuracy: 0.8924 - val_loss: 0.3162 - val_accuracy: 0.8948\n",
      "Epoch 44/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3260 - accuracy: 0.8935 - val_loss: 0.3147 - val_accuracy: 0.8982\n",
      "Epoch 45/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3235 - accuracy: 0.8942 - val_loss: 0.3078 - val_accuracy: 0.9009\n",
      "Epoch 46/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3226 - accuracy: 0.8950 - val_loss: 0.3082 - val_accuracy: 0.8991\n",
      "Epoch 47/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3197 - accuracy: 0.8963 - val_loss: 0.3053 - val_accuracy: 0.9025\n",
      "Epoch 48/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3177 - accuracy: 0.8973 - val_loss: 0.3057 - val_accuracy: 0.9031\n",
      "Epoch 49/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3159 - accuracy: 0.8985 - val_loss: 0.3033 - val_accuracy: 0.9037\n",
      "Epoch 50/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3156 - accuracy: 0.8985 - val_loss: 0.3027 - val_accuracy: 0.9040\n",
      "Epoch 51/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3144 - accuracy: 0.8993 - val_loss: 0.3018 - val_accuracy: 0.9045\n",
      "Epoch 52/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3130 - accuracy: 0.8997 - val_loss: 0.3009 - val_accuracy: 0.9049\n",
      "Epoch 53/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3126 - accuracy: 0.9002 - val_loss: 0.3084 - val_accuracy: 0.9000\n",
      "Epoch 54/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3132 - accuracy: 0.8999 - val_loss: 0.3023 - val_accuracy: 0.9036\n",
      "Epoch 55/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3109 - accuracy: 0.9010 - val_loss: 0.3001 - val_accuracy: 0.9049\n",
      "Epoch 56/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3113 - accuracy: 0.9010 - val_loss: 0.3006 - val_accuracy: 0.9047\n",
      "Epoch 57/100\n",
      "1502/1502 [==============================] - 40s 27ms/step - loss: 0.3104 - accuracy: 0.9012 - val_loss: 0.3007 - val_accuracy: 0.9049\n",
      "Epoch 58/100\n",
      "1502/1502 [==============================] - 38s 26ms/step - loss: 0.3100 - accuracy: 0.9014 - val_loss: 0.2999 - val_accuracy: 0.9053\n",
      "Epoch 59/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3101 - accuracy: 0.9016 - val_loss: 0.3003 - val_accuracy: 0.9048\n",
      "Epoch 60/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3102 - accuracy: 0.9014 - val_loss: 0.3003 - val_accuracy: 0.9049\n",
      "Epoch 61/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3097 - accuracy: 0.9017 - val_loss: 0.3001 - val_accuracy: 0.9052\n",
      "Epoch 62/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3092 - accuracy: 0.9018 - val_loss: 0.2998 - val_accuracy: 0.9051\n",
      "Epoch 63/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3100 - accuracy: 0.9018 - val_loss: 0.2996 - val_accuracy: 0.9052\n",
      "Epoch 64/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3087 - accuracy: 0.9021 - val_loss: 0.2995 - val_accuracy: 0.9053\n",
      "Epoch 65/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3089 - accuracy: 0.9019 - val_loss: 0.2993 - val_accuracy: 0.9053\n",
      "Epoch 66/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3089 - accuracy: 0.9020 - val_loss: 0.3003 - val_accuracy: 0.9046\n",
      "Epoch 67/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3089 - accuracy: 0.9020 - val_loss: 0.2992 - val_accuracy: 0.9053\n",
      "Epoch 68/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3091 - accuracy: 0.9020 - val_loss: 0.2990 - val_accuracy: 0.9053\n",
      "Epoch 69/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3083 - accuracy: 0.9024 - val_loss: 0.2995 - val_accuracy: 0.9052\n",
      "Epoch 70/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3089 - accuracy: 0.9020 - val_loss: 0.2993 - val_accuracy: 0.9052\n",
      "Epoch 71/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3081 - accuracy: 0.9024 - val_loss: 0.2993 - val_accuracy: 0.9053\n",
      "Epoch 72/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3080 - accuracy: 0.9024 - val_loss: 0.3022 - val_accuracy: 0.9041\n",
      "Epoch 73/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3083 - accuracy: 0.9022 - val_loss: 0.2987 - val_accuracy: 0.9054\n",
      "Epoch 74/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3081 - accuracy: 0.9022 - val_loss: 0.2984 - val_accuracy: 0.9054\n",
      "Epoch 75/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3079 - accuracy: 0.9024 - val_loss: 0.2990 - val_accuracy: 0.9053\n",
      "Epoch 76/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3075 - accuracy: 0.9024 - val_loss: 0.2983 - val_accuracy: 0.9055\n",
      "Epoch 77/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3080 - accuracy: 0.9025 - val_loss: 0.2997 - val_accuracy: 0.9050\n",
      "Epoch 78/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3077 - accuracy: 0.9025 - val_loss: 0.2984 - val_accuracy: 0.9053\n",
      "Epoch 79/100\n",
      "1502/1502 [==============================] - 37s 24ms/step - loss: 0.3083 - accuracy: 0.9021 - val_loss: 0.2987 - val_accuracy: 0.9055\n",
      "Epoch 80/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3079 - accuracy: 0.9025 - val_loss: 0.2987 - val_accuracy: 0.9055\n",
      "Epoch 81/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3077 - accuracy: 0.9024 - val_loss: 0.2982 - val_accuracy: 0.9054\n",
      "Epoch 82/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3074 - accuracy: 0.9024 - val_loss: 0.2991 - val_accuracy: 0.9053\n",
      "Epoch 83/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3070 - accuracy: 0.9027 - val_loss: 0.2978 - val_accuracy: 0.9055\n",
      "Epoch 84/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3072 - accuracy: 0.9025 - val_loss: 0.2986 - val_accuracy: 0.9052\n",
      "Epoch 85/100\n",
      "1502/1502 [==============================] - 38s 26ms/step - loss: 0.3071 - accuracy: 0.9025 - val_loss: 0.2978 - val_accuracy: 0.9055\n",
      "Epoch 86/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3069 - accuracy: 0.9027 - val_loss: 0.2980 - val_accuracy: 0.9055\n",
      "Epoch 87/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3067 - accuracy: 0.9027 - val_loss: 0.2983 - val_accuracy: 0.9056\n",
      "Epoch 88/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3068 - accuracy: 0.9026 - val_loss: 0.2974 - val_accuracy: 0.9057\n",
      "Epoch 89/100\n",
      "1502/1502 [==============================] - 39s 26ms/step - loss: 0.3069 - accuracy: 0.9025 - val_loss: 0.2980 - val_accuracy: 0.9055\n",
      "Epoch 90/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3064 - accuracy: 0.9027 - val_loss: 0.2975 - val_accuracy: 0.9056\n",
      "Epoch 91/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3068 - accuracy: 0.9025 - val_loss: 0.3001 - val_accuracy: 0.9047\n",
      "Epoch 92/100\n",
      "1502/1502 [==============================] - 36s 24ms/step - loss: 0.3066 - accuracy: 0.9027 - val_loss: 0.2978 - val_accuracy: 0.9056\n",
      "Epoch 93/100\n",
      "1502/1502 [==============================] - 38s 26ms/step - loss: 0.3070 - accuracy: 0.9025 - val_loss: 0.2976 - val_accuracy: 0.9056\n",
      "Epoch 94/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3060 - accuracy: 0.9029 - val_loss: 0.2978 - val_accuracy: 0.9055\n",
      "Epoch 95/100\n",
      "1502/1502 [==============================] - 38s 26ms/step - loss: 0.3064 - accuracy: 0.9028 - val_loss: 0.2967 - val_accuracy: 0.9058\n",
      "Epoch 96/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3057 - accuracy: 0.9028 - val_loss: 0.2972 - val_accuracy: 0.9052\n",
      "Epoch 97/100\n",
      "1502/1502 [==============================] - 38s 25ms/step - loss: 0.3059 - accuracy: 0.9029 - val_loss: 0.2964 - val_accuracy: 0.9058\n",
      "Epoch 98/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3057 - accuracy: 0.9029 - val_loss: 0.2968 - val_accuracy: 0.9056\n",
      "Epoch 99/100\n",
      "1502/1502 [==============================] - 38s 26ms/step - loss: 0.3061 - accuracy: 0.9027 - val_loss: 0.2973 - val_accuracy: 0.9058\n",
      "Epoch 100/100\n",
      "1502/1502 [==============================] - 37s 25ms/step - loss: 0.3052 - accuracy: 0.9030 - val_loss: 0.2974 - val_accuracy: 0.9058\n"
     ]
    }
   ],
   "source": [
    "#We run the below split to be able to have graphical model evaluation.\n",
    "\n",
    "#RNN Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units = 64, return_sequences = True, input_shape = input_shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(units = 32, input_shape = input_shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# Compile and Fit RNN Model\n",
    "# We set up our batch size and epochs to train for\n",
    "epochs = 100\n",
    "bs = 100\n",
    "# We set our optimzer of choice and compile our model.\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "# We Train the network\n",
    "history = model.fit(trainX, trainY, epochs = epochs, batch_size = bs, validation_data=(validationX, validationY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.6442 - accuracy: 0.6590\n",
      "Epoch 2/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.6425 - accuracy: 0.6590\n",
      "Epoch 3/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.6390 - accuracy: 0.6598\n",
      "Epoch 4/100\n",
      "2086/2086 [==============================] - 49s 24ms/step - loss: 0.3899 - accuracy: 0.8651\n",
      "Epoch 5/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3622 - accuracy: 0.8796\n",
      "Epoch 6/100\n",
      "2086/2086 [==============================] - 48s 23ms/step - loss: 0.3580 - accuracy: 0.8814\n",
      "Epoch 7/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3549 - accuracy: 0.8825\n",
      "Epoch 8/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3524 - accuracy: 0.88400s - loss: 0.3524 - accuracy: 0.88\n",
      "Epoch 9/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3520 - accuracy: 0.8836\n",
      "Epoch 10/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3504 - accuracy: 0.8847\n",
      "Epoch 11/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3508 - accuracy: 0.8848\n",
      "Epoch 12/100\n",
      "2086/2086 [==============================] - 48s 23ms/step - loss: 0.3486 - accuracy: 0.8857\n",
      "Epoch 13/100\n",
      "2086/2086 [==============================] - 48s 23ms/step - loss: 0.3475 - accuracy: 0.8859\n",
      "Epoch 14/100\n",
      "2086/2086 [==============================] - 48s 23ms/step - loss: 0.3475 - accuracy: 0.8865\n",
      "Epoch 15/100\n",
      "2086/2086 [==============================] - 48s 23ms/step - loss: 0.3473 - accuracy: 0.8865\n",
      "Epoch 16/100\n",
      "2086/2086 [==============================] - 49s 24ms/step - loss: 0.3461 - accuracy: 0.8866\n",
      "Epoch 17/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3457 - accuracy: 0.8868\n",
      "Epoch 18/100\n",
      "2086/2086 [==============================] - 48s 23ms/step - loss: 0.3448 - accuracy: 0.8870\n",
      "Epoch 19/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3444 - accuracy: 0.8871\n",
      "Epoch 20/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3441 - accuracy: 0.8870\n",
      "Epoch 21/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3434 - accuracy: 0.8875\n",
      "Epoch 22/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3428 - accuracy: 0.8877\n",
      "Epoch 23/100\n",
      "2086/2086 [==============================] - 48s 23ms/step - loss: 0.3421 - accuracy: 0.8880\n",
      "Epoch 24/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3413 - accuracy: 0.8879\n",
      "Epoch 25/100\n",
      "2086/2086 [==============================] - 51s 25ms/step - loss: 0.3406 - accuracy: 0.8884\n",
      "Epoch 26/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3393 - accuracy: 0.8887\n",
      "Epoch 27/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3380 - accuracy: 0.8891\n",
      "Epoch 28/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3343 - accuracy: 0.8901\n",
      "Epoch 29/100\n",
      "2086/2086 [==============================] - 49s 24ms/step - loss: 0.3291 - accuracy: 0.8920\n",
      "Epoch 30/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3227 - accuracy: 0.8944\n",
      "Epoch 31/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3175 - accuracy: 0.8969\n",
      "Epoch 32/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3142 - accuracy: 0.8988\n",
      "Epoch 33/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3136 - accuracy: 0.8994\n",
      "Epoch 34/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3123 - accuracy: 0.8998\n",
      "Epoch 35/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3104 - accuracy: 0.9006\n",
      "Epoch 36/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3106 - accuracy: 0.9006\n",
      "Epoch 37/100\n",
      "2086/2086 [==============================] - 52s 25ms/step - loss: 0.3095 - accuracy: 0.9011\n",
      "Epoch 38/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3090 - accuracy: 0.9014\n",
      "Epoch 39/100\n",
      "2086/2086 [==============================] - 55s 27ms/step - loss: 0.3093 - accuracy: 0.9012\n",
      "Epoch 40/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3088 - accuracy: 0.9013\n",
      "Epoch 41/100\n",
      "2086/2086 [==============================] - 52s 25ms/step - loss: 0.3087 - accuracy: 0.9012\n",
      "Epoch 42/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3081 - accuracy: 0.9017\n",
      "Epoch 43/100\n",
      "2086/2086 [==============================] - 51s 25ms/step - loss: 0.3085 - accuracy: 0.9017\n",
      "Epoch 44/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3082 - accuracy: 0.90150s - loss: 0.3081 - accuracy\n",
      "Epoch 45/100\n",
      "2086/2086 [==============================] - 53s 25ms/step - loss: 0.3072 - accuracy: 0.9018\n",
      "Epoch 46/100\n",
      "2086/2086 [==============================] - 63s 30ms/step - loss: 0.3076 - accuracy: 0.9019\n",
      "Epoch 47/100\n",
      "2086/2086 [==============================] - 53s 25ms/step - loss: 0.3078 - accuracy: 0.9015\n",
      "Epoch 48/100\n",
      "2086/2086 [==============================] - 53s 25ms/step - loss: 0.3075 - accuracy: 0.9018\n",
      "Epoch 49/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3076 - accuracy: 0.9018\n",
      "Epoch 50/100\n",
      "2086/2086 [==============================] - 52s 25ms/step - loss: 0.3068 - accuracy: 0.9022\n",
      "Epoch 51/100\n",
      "2086/2086 [==============================] - 53s 25ms/step - loss: 0.3069 - accuracy: 0.9022\n",
      "Epoch 52/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3061 - accuracy: 0.9022\n",
      "Epoch 53/100\n",
      "2086/2086 [==============================] - 49s 24ms/step - loss: 0.3063 - accuracy: 0.9021\n",
      "Epoch 54/100\n",
      "2086/2086 [==============================] - 51s 25ms/step - loss: 0.3060 - accuracy: 0.9024\n",
      "Epoch 55/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3063 - accuracy: 0.9022\n",
      "Epoch 56/100\n",
      "2086/2086 [==============================] - 52s 25ms/step - loss: 0.3058 - accuracy: 0.9024\n",
      "Epoch 57/100\n",
      "2086/2086 [==============================] - 51s 25ms/step - loss: 0.3052 - accuracy: 0.9025\n",
      "Epoch 58/100\n",
      "2086/2086 [==============================] - 52s 25ms/step - loss: 0.3058 - accuracy: 0.9025\n",
      "Epoch 59/100\n",
      "2086/2086 [==============================] - 57s 27ms/step - loss: 0.3052 - accuracy: 0.9025\n",
      "Epoch 60/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3071 - accuracy: 0.9018\n",
      "Epoch 61/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3050 - accuracy: 0.9025\n",
      "Epoch 62/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3053 - accuracy: 0.9025\n",
      "Epoch 63/100\n",
      "2086/2086 [==============================] - 58s 28ms/step - loss: 0.3050 - accuracy: 0.9027\n",
      "Epoch 64/100\n",
      "2086/2086 [==============================] - 52s 25ms/step - loss: 0.3051 - accuracy: 0.9026\n",
      "Epoch 65/100\n",
      "2086/2086 [==============================] - 52s 25ms/step - loss: 0.3047 - accuracy: 0.9028\n",
      "Epoch 66/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3050 - accuracy: 0.9026\n",
      "Epoch 67/100\n",
      "2086/2086 [==============================] - 52s 25ms/step - loss: 0.3046 - accuracy: 0.9028\n",
      "Epoch 68/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3045 - accuracy: 0.9028\n",
      "Epoch 69/100\n",
      "2086/2086 [==============================] - 51s 25ms/step - loss: 0.3046 - accuracy: 0.9027\n",
      "Epoch 70/100\n",
      "2086/2086 [==============================] - 52s 25ms/step - loss: 0.3046 - accuracy: 0.9028\n",
      "Epoch 71/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3046 - accuracy: 0.9027\n",
      "Epoch 72/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3045 - accuracy: 0.9028\n",
      "Epoch 73/100\n",
      "2086/2086 [==============================] - 51s 25ms/step - loss: 0.3046 - accuracy: 0.9028\n",
      "Epoch 74/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3041 - accuracy: 0.9030\n",
      "Epoch 75/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3044 - accuracy: 0.9027\n",
      "Epoch 76/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3043 - accuracy: 0.9029\n",
      "Epoch 77/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3037 - accuracy: 0.9029\n",
      "Epoch 78/100\n",
      "2086/2086 [==============================] - 51s 25ms/step - loss: 0.3044 - accuracy: 0.9027\n",
      "Epoch 79/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3038 - accuracy: 0.9030\n",
      "Epoch 80/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3041 - accuracy: 0.90281s - l\n",
      "Epoch 81/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3037 - accuracy: 0.9029\n",
      "Epoch 82/100\n",
      "2086/2086 [==============================] - 48s 23ms/step - loss: 0.3038 - accuracy: 0.9029\n",
      "Epoch 83/100\n",
      "2086/2086 [==============================] - 49s 24ms/step - loss: 0.3038 - accuracy: 0.9029\n",
      "Epoch 84/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3035 - accuracy: 0.9030\n",
      "Epoch 85/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3036 - accuracy: 0.9030\n",
      "Epoch 86/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3036 - accuracy: 0.90300s - loss: 0.3036 - accuracy: \n",
      "Epoch 87/100\n",
      "2086/2086 [==============================] - 53s 25ms/step - loss: 0.3035 - accuracy: 0.9030\n",
      "Epoch 88/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3031 - accuracy: 0.9031\n",
      "Epoch 89/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3036 - accuracy: 0.9030\n",
      "Epoch 90/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3033 - accuracy: 0.9031\n",
      "Epoch 91/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3037 - accuracy: 0.9029\n",
      "Epoch 92/100\n",
      "2086/2086 [==============================] - 51s 25ms/step - loss: 0.3032 - accuracy: 0.9030\n",
      "Epoch 93/100\n",
      "2086/2086 [==============================] - 55s 26ms/step - loss: 0.3034 - accuracy: 0.9030\n",
      "Epoch 94/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3031 - accuracy: 0.9031\n",
      "Epoch 95/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3033 - accuracy: 0.9030\n",
      "Epoch 96/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3033 - accuracy: 0.9031\n",
      "Epoch 97/100\n",
      "2086/2086 [==============================] - 50s 24ms/step - loss: 0.3030 - accuracy: 0.9031\n",
      "Epoch 98/100\n",
      "2086/2086 [==============================] - 49s 23ms/step - loss: 0.3027 - accuracy: 0.9030\n",
      "Epoch 99/100\n",
      "2086/2086 [==============================] - 49s 24ms/step - loss: 0.3029 - accuracy: 0.9032\n",
      "Epoch 100/100\n",
      "2086/2086 [==============================] - 51s 24ms/step - loss: 0.3026 - accuracy: 0.9031\n"
     ]
    }
   ],
   "source": [
    "# We finally train the model on the entire dataset as we know it performs well no matter which fold was used.\n",
    "# This allows us to have 20% more data to train on.\n",
    "\n",
    "#RNN Model entire dataset training\n",
    "model_final = Sequential()\n",
    "model_final.add(LSTM(units = 64, return_sequences = True, input_shape = input_shape))\n",
    "model_final.add(Activation(\"relu\"))\n",
    "model_final.add(Dropout(0.1))\n",
    "model_final.add(LSTM(units = 32, input_shape = input_shape))\n",
    "model_final.add(Activation(\"relu\"))\n",
    "model_final.add(Dropout(0.1))\n",
    "model_final.add(Dense(1))\n",
    "model_final.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# Compile and Fit RNN Model\n",
    "# We set up our batch size and epochs to train for\n",
    "epochs = 100\n",
    "bs = 100\n",
    "# We set our optimzer of choice and compile our model.\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model_final.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "# We Train the network\n",
    "history = model_final.fit(data_array_resh, lb.fit_transform(labels_array), epochs = epochs, batch_size = bs)\n",
    "# Save the model\n",
    "model_final.save(\"RNN_model_Final\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation \n",
    "\n",
    "* To evaluate our models, we use a combination of accuracy charts, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  <= 6 years       0.88      0.99      0.93     27545\n",
      "   > 6 years       0.96      0.74      0.84     14159\n",
      "\n",
      "    accuracy                           0.90     41704\n",
      "   macro avg       0.92      0.86      0.88     41704\n",
      "weighted avg       0.91      0.90      0.90     41704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN Model Evaluation\n",
    "# Plot the training and validation accuracy and loss\n",
    "epochs = 100\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, label='Training acc')\n",
    "plt.plot(epochs, val_acc, label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Prediction on test dataset Accuracy using classification_report\n",
    "predictions = (model.predict(testX) > 0.5).astype(\"int32\")\n",
    "print(classification_report(testY, predictions, target_names = ['<= 6 years','> 6 years']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0     30278\n",
      "3.0     27792\n",
      "4.0     25501\n",
      "5.0     23284\n",
      "6.0     21368\n",
      "7.0     19812\n",
      "8.0     19234\n",
      "9.0     17277\n",
      "10.0    14781\n",
      "1.0      9190\n",
      "Name: SRV_TIME_YR, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We start by reading the extracted csv file that has all the variables deemed necessary to answer our question.\n",
    "cancer_data_regression = pd.read_csv('breast_cancer_completeV1.csv')\n",
    "\n",
    "# We drop any rows with no values\n",
    "cancer_data_regression.dropna(inplace=True)\n",
    "\n",
    "#removing male gender because negligeable representation\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['SEX'] == 1].index, inplace = True)\n",
    "#removing sex column because only 1 value there now\n",
    "cancer_data_regression = cancer_data_regression.drop(['SEX'], axis='columns')\n",
    "\n",
    "#reformate no_surg column to boolean or surgery (0) or no surgery (1)\n",
    "cancer_data_regression[\"NO_SURG\"].replace({1: 1, 2: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}, inplace=True)\n",
    "\n",
    "#drop rows with unkown age =99 and drop under 30 \n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['AGE_1REC'] < 7].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['AGE_1REC'] == 99].index, inplace = True)\n",
    "\n",
    "#combine other group for rac_reca and add unkown to other\n",
    "cancer_data_regression[\"RAC_RECA\"].replace({7: 2, 9: 2, 3: 2}, inplace=True)\n",
    "\n",
    "#combine marital stauts to has a partner or single\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['MAR_STAT'] == 9].index, inplace = True)\n",
    "cancer_data_regression[\"MAR_STAT\"].replace({3: 1, 4: 1, 5: 1, 6: 2}, inplace=True)\n",
    "\n",
    "#get rid of any rows that are not alive or dead because of breast cancer\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50060].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50300].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50130].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 27030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50080].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50160].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50050].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 37000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 22030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 27040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50210].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50051].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50120].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21100].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 35021].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50100].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50150].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21050].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 31010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 29020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21130].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 23000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50070].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 29010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 27010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 38000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 32020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 34000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21080].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 41000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 33040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 22020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21072].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 29030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50220].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 35011].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50200].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50180].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 33010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 30000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 25010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 20050].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50140].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21071].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50170].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 24000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 35043].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 27070].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 27050].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 27060].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 35012].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21110].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 35031].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 35023].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 25020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 20070].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 35013].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 29040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 20030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 20080].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 28010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 20040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50190].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21120].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50230].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21090].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 27020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21060].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 21010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 20060].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 35022].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 20100].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 32010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 22010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50110].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 20020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 35041].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 50090].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUB'] == 22060].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50060].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50300].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50130].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 27030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50080].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50160].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50050].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 37000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 22030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 27040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50210].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50051].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50120].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21100].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 35021].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50100].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50150].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21050].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 31010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 29020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21130].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 23000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50070].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 29010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 27010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 38000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 32020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 34000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21080].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 41000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 33040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 22020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21072].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 29030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50220].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 35011].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50200].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50180].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 33010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 30000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 25010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 20050].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50140].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21071].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50170].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 24000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 35043].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 27070].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 27050].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 27060].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 35012].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21110].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 35031].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 35023].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 25020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 20070].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 35013].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 29040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 20030].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 20080].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 28010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 20040].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50190].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21120].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50230].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21090].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 27020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21060].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 21010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 20060].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 35022].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 20100].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 32010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 22010].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50110].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50000].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 20020].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 35041].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 50090].index, inplace = True)\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['CODPUBKM'] == 22060].index, inplace = True)\n",
    "cancer_data_regression = cancer_data_regression.drop(['CODPUB'], axis='columns')\n",
    "cancer_data_regression = cancer_data_regression.drop(['CODPUBKM'], axis='columns')\n",
    "\n",
    "#drop unkown survival time\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['SRV_TIME_MON'] == 9999].index, inplace = True)\n",
    "\n",
    "#group insured and insured/no specifics\n",
    "cancer_data_regression[\"INSREC_PUB\"].replace({4: 3}, inplace=True)\n",
    "\n",
    "#get rid of rows with unkown stage in ADJAJCCSTG\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['ADJAJCCSTG'] == 99].index, inplace = True)\n",
    "\n",
    "#get rid of rows with unkown tumor counts MALIGCOUNT\n",
    "cancer_data_regression.drop(cancer_data_regression[cancer_data_regression['MALIGCOUNT'] == 99].index, inplace = True)\n",
    "\n",
    "#set the conditions for the binary split and perform the split\n",
    "conditions = [(cancer_data_regression['SRV_TIME_MON'] < 73), (cancer_data_regression['SRV_TIME_MON'] > 72)]\n",
    "choices = ['0', '1']\n",
    "cancer_data_regression['Survival_category'] = np.select(conditions, choices, default=\"N/A\")\n",
    "\n",
    "#coverting SRV_TIME_MON into Years \"SRV_TIME_YR\"\n",
    "cancer_data_regression[\"SRV_TIME_YR\"] = cancer_data_regression[\"SRV_TIME_MON\"] / 12\n",
    "cancer_data_regression['SRV_TIME_YR'] = cancer_data_regression['SRV_TIME_YR'].apply(np.ceil)\n",
    "\n",
    "#Combine 0 and 1 years survival data\n",
    "cancer_data_regression[\"SRV_TIME_YR\"].replace({0: 1}, inplace=True)\n",
    "print(cancer_data_regression['SRV_TIME_YR'].value_counts())\n",
    "\n",
    "#drop the survival time in months as it is no longer relevant\n",
    "cancer_data_regression = cancer_data_regression.drop(['SRV_TIME_MON'], axis='columns')\n",
    "\n",
    "#displaying part of the dataframe if further investigation is necessary by uncommenting the line of code below\n",
    "#cancer_data_regression\n",
    "\n",
    "#create dummy variables for the categorical variables\n",
    "dummies = pd.get_dummies(cancer_data_regression, columns = ['MAR_STAT', 'SEQ_NUM', 'PRIMSITE', 'LATERAL', 'GRADE', 'SURGPRIF', 'SURGSITF', \n",
    "                                                'NO_SURG', \n",
    "                                                'AGE_1REC', 'BEHTREND', 'RAC_RECA', 'STAT_REC', 'ERSTATUS',\n",
    "                                                'PRSTATUS', 'INSREC_PUB', 'ADJTM_6VALUE', 'ADJNM_6VALUE', 'ADJM_6VALUE', \n",
    "                                                'ADJAJCCSTG', 'her2', 'brst_sub', 'MALIGCOUNT', 'BENBORDCOUNT',\n",
    "                                                'RADIATION','RAD_SURG_SEQ','CHEMO'])\n",
    "\n",
    "#final dataframe\n",
    "cancer_data_regression_final = dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA For Logistic Regression- Less than 6 Years of Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression for less than 6 years\n",
    "data_0 = cancer_data_regression_final[cancer_data_regression_final['Survival_category'] == '0']\n",
    "#uncomment code below to look at dataset\n",
    "#data_0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for models\n",
    "# We first need to split our dataset into training and testing (X0 and Y0)\n",
    "X0=data_0.drop(columns=['SRV_TIME_YR'])\n",
    "Y0=data_0['SRV_TIME_YR']\n",
    "X0_train, X0_test,Y0_train,Y0_test = train_test_split(X0,Y0,test_size =0.2, random_state =0)\n",
    "\n",
    "#standarizing the data for PCA Analsyis\n",
    "scaler=StandardScaler()\n",
    "#fitting the on training set only\n",
    "scaler.fit(X0_train)\n",
    "#apply transformation to both training set and the test set\n",
    "X0_train = scaler.transform(X0_train)\n",
    "X0_test = scaler.transform(X0_test)\n",
    "#Please uncomment code below to see what the scaler transformation does\n",
    "#X0_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109930, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA Analysis\n",
    "#Make an instance of the Model- fitting it ONLY on training set\n",
    "pca = PCA(n_components = 100)\n",
    "pca.fit(X0_train)\n",
    "#Applying the PCA transforming to both training set and test set\n",
    "X0_train = pca.transform(X0_train)\n",
    "X0_test = pca.transform(X0_test)\n",
    "X0_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The following code constructs the Scree plot\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    " \n",
    "plt.bar(x=range(50,len(per_var)+50), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Less than 6 years of survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.24\n"
     ]
    }
   ],
   "source": [
    "log_model0 = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "log_model0.fit(X0_train, Y0_train)\n",
    "log_model0_predict = log_model0.predict(X0_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(log_model0.score(X0_test, Y0_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 593  833  177   86   64  110]\n",
      " [ 291 3795  665  447  352  515]\n",
      " [ 168 3296  712  409  362  617]\n",
      " [ 125 2876  652  439  383  604]\n",
      " [  78 2505  621  419  401  624]\n",
      " [  61 2280  582  353  340  648]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.45      0.32      0.37      1863\n",
      "         2.0       0.24      0.63      0.35      6065\n",
      "         3.0       0.21      0.13      0.16      5564\n",
      "         4.0       0.20      0.09      0.12      5079\n",
      "         5.0       0.21      0.09      0.12      4648\n",
      "         6.0       0.21      0.15      0.18      4264\n",
      "\n",
      "    accuracy                           0.24     27483\n",
      "   macro avg       0.25      0.23      0.22     27483\n",
      "weighted avg       0.23      0.24      0.21     27483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Metrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y0_test, log_model0_predict)\n",
    "print(confusion_matrix)\n",
    "#Precision, Recall, F-measure and support\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y0_test, log_model0_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Logistic Regression- More than 6 Years of Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression for less than 6 years\n",
    "data_1 = cancer_data_regression_final[cancer_data_regression_final['Survival_category'] == '1']\n",
    "#uncomment code below to look at dataset\n",
    "#data_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.76873588,  0.76873588, ..., -0.02335113,\n",
       "        -1.44305965,  1.44305965],\n",
       "       [ 0.        , -0.76873588,  0.76873588, ..., -0.02335113,\n",
       "         0.69297205, -0.69297205],\n",
       "       [ 0.        ,  1.3008369 , -1.3008369 , ..., -0.02335113,\n",
       "         0.69297205, -0.69297205],\n",
       "       ...,\n",
       "       [ 0.        ,  1.3008369 , -1.3008369 , ..., -0.02335113,\n",
       "         0.69297205, -0.69297205],\n",
       "       [ 0.        ,  1.3008369 , -1.3008369 , ..., -0.02335113,\n",
       "         0.69297205, -0.69297205],\n",
       "       [ 0.        , -0.76873588,  0.76873588, ..., -0.02335113,\n",
       "         0.69297205, -0.69297205]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparation for models\n",
    "# We first need to split our dataset into training and testing (X1 and Y1)\n",
    "X1=data_1.drop(columns=['SRV_TIME_YR'])\n",
    "Y1=data_1['SRV_TIME_YR']\n",
    "X1_train, X1_test,Y1_train,Y1_test = train_test_split(X1,Y1,test_size =0.2, random_state =0)\n",
    "\n",
    "#standarizing the data for PCA Analsyis\n",
    "scaler=StandardScaler()\n",
    "#fitting the on training set only\n",
    "scaler.fit(X1_train)\n",
    "#apply transformation to both training set and the test set\n",
    "X1_train = scaler.transform(X1_train)\n",
    "X1_test = scaler.transform(X1_test)\n",
    "#Please uncomment code below to see what the scaler transformation does\n",
    "X1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56883, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA Analysis\n",
    "#Make an instance of the Model- fitting it ONLY on training set\n",
    "pca = PCA(n_components = 100)\n",
    "pca.fit(X1_train)\n",
    "#Applying the PCA transforming to both training set and test set\n",
    "X1_train = pca.transform(X1_train)\n",
    "X1_test = pca.transform(X1_test)\n",
    "X1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The following code constructs the Scree plot\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    " \n",
    "plt.bar(x=range(50,len(per_var)+50), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression- More than 6 years of survivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.52\n"
     ]
    }
   ],
   "source": [
    "log_model1 = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "log_model1.fit(X1_train, Y1_train)\n",
    "log_model1_predict = log_model1.predict(X1_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(log_model1.score(X1_test, Y1_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3524  382   81   29]\n",
      " [  55 2943  700  236]\n",
      " [  21 2380  693  263]\n",
      " [  10 2039  601  264]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         7.0       0.98      0.88      0.92      4016\n",
      "         8.0       0.38      0.75      0.50      3934\n",
      "         9.0       0.33      0.21      0.26      3357\n",
      "        10.0       0.33      0.09      0.14      2914\n",
      "\n",
      "    accuracy                           0.52     14221\n",
      "   macro avg       0.51      0.48      0.46     14221\n",
      "weighted avg       0.53      0.52      0.49     14221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion Metrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y1_test, log_model1_predict)\n",
    "print(confusion_matrix)\n",
    "#Precision, Recall, F-measure and support\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y1_test, log_model1_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by reading the extracted csv file that has all the variables deemed necessary to answer our question.\n",
    "cancer_data_Bayesian = pd.read_csv('breast_cancer_completeV1.csv')\n",
    "#please uncomment this line of code to view the original dataset\n",
    "#cancer_data_Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop any rows with no values\n",
    "cancer_data_Bayesian.dropna(inplace=True)\n",
    "\n",
    "#removing male gender because negligeable representation\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['SEX'] == 1].index, inplace = True)\n",
    "#removing sex column because only 1 value there now\n",
    "cancer_data_Bayesian = cancer_data_Bayesian.drop(['SEX'], axis='columns')\n",
    "\n",
    "#reformate no_surg column to boolean or surgery (0) or no surgery (1)\n",
    "cancer_data_Bayesian[\"NO_SURG\"].replace({1: 1, 2: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}, inplace=True)\n",
    "\n",
    "#drop rows with unkown age =99 and drop under 30 \n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['AGE_1REC'] < 7].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['AGE_1REC'] == 99].index, inplace = True)\n",
    "\n",
    "#combine other group for rac_reca and add unkown to other\n",
    "cancer_data_Bayesian[\"RAC_RECA\"].replace({7: 2, 9: 2, 3: 2}, inplace=True)\n",
    "\n",
    "#combine marital stauts to has a partner or single\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['MAR_STAT'] == 9].index, inplace = True)\n",
    "cancer_data_Bayesian[\"MAR_STAT\"].replace({3: 1, 4: 1, 5: 1, 6: 2}, inplace=True)\n",
    "\n",
    "#get rid of any rows that are not alive or dead because of breast cancer\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50060].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50300].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50130].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 27030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50080].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50160].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50050].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 37000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 22030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 27040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50210].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50051].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50120].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21100].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 35021].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50100].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50150].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21050].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 31010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 29020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21130].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 23000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50070].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 29010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 27010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 38000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 32020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 34000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21080].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 41000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 33040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 22020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21072].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 29030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50220].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 35011].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50200].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50180].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 33010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 30000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 25010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 20050].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50140].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21071].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50170].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 24000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 35043].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 27070].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 27050].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 27060].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 35012].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21110].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 35031].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 35023].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 25020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 20070].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 35013].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 29040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 20030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 20080].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 28010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 20040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50190].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21120].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50230].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21090].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 27020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21060].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 21010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 20060].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 35022].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 20100].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 32010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 22010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50110].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 20020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 35041].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 50090].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUB'] == 22060].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50060].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50300].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50130].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 27030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50080].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50160].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50050].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 37000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 22030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 27040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50210].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50051].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50120].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21100].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 35021].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50100].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50150].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21050].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 31010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 29020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21130].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 23000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50070].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 29010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 27010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 38000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 32020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 34000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21080].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 41000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 33040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 22020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21072].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 29030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50220].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 35011].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50200].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50180].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 33010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 30000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 25010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 20050].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50140].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21071].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50170].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 24000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 35043].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 27070].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 27050].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 27060].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 35012].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21110].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 35031].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 35023].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 25020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 20070].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 35013].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 29040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 20030].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 20080].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 28010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 20040].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50190].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21120].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50230].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21090].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 27020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21060].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 21010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 20060].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 35022].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 20100].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 32010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 22010].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50110].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50000].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 20020].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 35041].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 50090].index, inplace = True)\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['CODPUBKM'] == 22060].index, inplace = True)\n",
    "cancer_data_Bayesian = cancer_data_Bayesian.drop(['CODPUB'], axis='columns')\n",
    "cancer_data_Bayesian = cancer_data_Bayesian.drop(['CODPUBKM'], axis='columns')\n",
    "\n",
    "#drop unkown survival time\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['SRV_TIME_MON'] == 9999].index, inplace = True)\n",
    "\n",
    "#group insured and insured/no specifics\n",
    "cancer_data_Bayesian[\"INSREC_PUB\"].replace({4: 3}, inplace=True)\n",
    "\n",
    "#get rid of rows with unkown stage in ADJAJCCSTG\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['ADJAJCCSTG'] == 99].index, inplace = True)\n",
    "\n",
    "#get rid of rows with unkown tumor counts MALIGCOUNT\n",
    "cancer_data_Bayesian.drop(cancer_data_Bayesian[cancer_data_Bayesian['MALIGCOUNT'] == 99].index, inplace = True)\n",
    "\n",
    "#coverting SRV_TIME_MON into Years \"SRV_TIME_YR\"\n",
    "cancer_data_Bayesian[\"SRV_TIME_YR\"] = cancer_data_Bayesian[\"SRV_TIME_MON\"] / 12\n",
    "cancer_data_Bayesian['SRV_TIME_YR'] = cancer_data_Bayesian['SRV_TIME_YR'].apply(np.ceil)\n",
    "\n",
    "#drop the survival time in months as it is no longer relevant\n",
    "cancer_data_Bayesian = cancer_data_Bayesian.drop(['SRV_TIME_MON'], axis='columns')\n",
    "\n",
    "#we finaly only keep the following variables as we are trying to get a better understadning of Bayesian networks\n",
    "#and want to reduce the complexity we face\n",
    "bn_cancer_data = cancer_data_Bayesian[[\"SRV_TIME_YR\", \"NO_SURG\", \"RADIATION\", \"RAD_SURG_SEQ\", \"CHEMO\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we set up the structure and edges of our model based on our understanding.\n",
    "#it would be better to have a subject expert decide these structures.\n",
    "sm_cancer = StructureModel()\n",
    "\n",
    "sm_cancer.add_edges_from([\n",
    "    ('NO_SURG', 'SRV_TIME_YR'),\n",
    "    ('RADIATION', 'SRV_TIME_YR'),\n",
    "    ('RAD_SURG_SEQ', 'SRV_TIME_YR'),\n",
    "    ('CHEMO', 'SRV_TIME_YR'),\n",
    "    ('RADIATION', 'RAD_SURG_SEQ'),\n",
    "    ('NO_SURG', 'RAD_SURG_SEQ')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split our dataset into training and testing\n",
    "train, test = train_test_split(bn_cancer_data, train_size = 0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fadyn\\AppData\\Roaming\\Python\\Python38\\site-packages\\pgmpy\\models\\BayesianModel.py:8: FutureWarning: BayesianModel has been renamed to BayesianNetwork. Please use BayesianNewtork class, BayesianModel will be removed in future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#we fit our model\n",
    "bn_cancer = BayesianNetwork(sm_cancer)\n",
    "bn_cancer = bn_cancer.fit_node_states(bn_cancer_data)\n",
    "bn_cancer = bn_cancer.fit_cpds(train, method=\"BayesianEstimator\", bayes_prior=\"K2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6041227345259322\n"
     ]
    }
   ],
   "source": [
    "#check auc to make sure the results aren't horrible\n",
    "roc, auc = roc_auc(bn_cancer, test, \"SRV_TIME_YR\")\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>CHEMO</th>\n",
       "      <th colspan=\"10\" halign=\"left\">0.0</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NO_SURG</th>\n",
       "      <th colspan=\"10\" halign=\"left\">0</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RADIATION</th>\n",
       "      <th colspan=\"8\" halign=\"left\">0.0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1.0</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">7.0</th>\n",
       "      <th colspan=\"8\" halign=\"left\">8.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD_SURG_SEQ</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>...</th>\n",
       "      <th>7.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>9.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRV_TIME_YR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.028819</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.134185</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.146974</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.127382</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.142651</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.124702</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.145533</td>\n",
       "      <td>0.080952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.114461</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.108069</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>0.105662</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.108069</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>0.099728</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083573</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>0.099137</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083573</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>0.089543</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.073487</td>\n",
       "      <td>0.080952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.075457</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.074928</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "CHEMO              0.0                                                    \\\n",
       "NO_SURG              0                                                     \n",
       "RADIATION          0.0                                                     \n",
       "RAD_SURG_SEQ       0.0       2.0       3.0       4.0       5.0       6.0   \n",
       "SRV_TIME_YR                                                                \n",
       "0.0           0.000924  0.090909  0.071429  0.090909  0.090909  0.090909   \n",
       "1.0           0.028819  0.090909  0.071429  0.090909  0.090909  0.090909   \n",
       "2.0           0.134185  0.090909  0.071429  0.090909  0.090909  0.090909   \n",
       "3.0           0.127382  0.090909  0.142857  0.090909  0.090909  0.090909   \n",
       "4.0           0.124702  0.090909  0.142857  0.090909  0.090909  0.090909   \n",
       "5.0           0.114461  0.090909  0.142857  0.090909  0.090909  0.090909   \n",
       "6.0           0.105662  0.090909  0.071429  0.090909  0.090909  0.090909   \n",
       "7.0           0.099728  0.090909  0.071429  0.090909  0.090909  0.090909   \n",
       "8.0           0.099137  0.090909  0.071429  0.090909  0.090909  0.090909   \n",
       "9.0           0.089543  0.090909  0.071429  0.090909  0.090909  0.090909   \n",
       "10.0          0.075457  0.090909  0.071429  0.090909  0.090909  0.090909   \n",
       "\n",
       "CHEMO                                                 ...       1.0            \\\n",
       "NO_SURG                                               ...         1             \n",
       "RADIATION                              1.0            ...       7.0             \n",
       "RAD_SURG_SEQ       7.0       9.0       0.0       2.0  ...       7.0       9.0   \n",
       "SRV_TIME_YR                                           ...                       \n",
       "0.0           0.090909  0.083333  0.001441  0.004762  ...  0.090909  0.090909   \n",
       "1.0           0.090909  0.083333  0.031700  0.033333  ...  0.090909  0.090909   \n",
       "2.0           0.090909  0.083333  0.146974  0.133333  ...  0.090909  0.090909   \n",
       "3.0           0.090909  0.083333  0.142651  0.119048  ...  0.090909  0.090909   \n",
       "4.0           0.090909  0.166667  0.145533  0.080952  ...  0.090909  0.090909   \n",
       "5.0           0.090909  0.083333  0.108069  0.100000  ...  0.090909  0.090909   \n",
       "6.0           0.090909  0.083333  0.108069  0.109524  ...  0.090909  0.090909   \n",
       "7.0           0.090909  0.083333  0.083573  0.100000  ...  0.090909  0.090909   \n",
       "8.0           0.090909  0.083333  0.083573  0.109524  ...  0.090909  0.090909   \n",
       "9.0           0.090909  0.083333  0.073487  0.080952  ...  0.090909  0.090909   \n",
       "10.0          0.090909  0.083333  0.074928  0.128571  ...  0.090909  0.090909   \n",
       "\n",
       "CHEMO                                                                  \\\n",
       "NO_SURG                                                                 \n",
       "RADIATION       8.0                                                     \n",
       "RAD_SURG_SEQ    0.0       2.0       3.0       4.0       5.0       6.0   \n",
       "SRV_TIME_YR                                                             \n",
       "0.0           0.040  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "1.0           0.192  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "2.0           0.144  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "3.0           0.104  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "4.0           0.176  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "5.0           0.120  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "6.0           0.048  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "7.0           0.072  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "8.0           0.040  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "9.0           0.024  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "10.0          0.040  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "\n",
       "CHEMO                             \n",
       "NO_SURG                           \n",
       "RADIATION                         \n",
       "RAD_SURG_SEQ       7.0       9.0  \n",
       "SRV_TIME_YR                       \n",
       "0.0           0.090909  0.090909  \n",
       "1.0           0.090909  0.090909  \n",
       "2.0           0.090909  0.090909  \n",
       "3.0           0.090909  0.090909  \n",
       "4.0           0.090909  0.090909  \n",
       "5.0           0.090909  0.090909  \n",
       "6.0           0.090909  0.090909  \n",
       "7.0           0.090909  0.090909  \n",
       "8.0           0.090909  0.090909  \n",
       "9.0           0.090909  0.090909  \n",
       "10.0          0.090909  0.090909  \n",
       "\n",
       "[11 rows x 288 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call cpds for survival time in years to see how different treatment methods affect its probability\n",
    "cond_prob_dist = bn_cancer.cpds['SRV_TIME_YR']\n",
    "cond_prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0.053763440860215075,\n",
       " 1.0: 0.23655913978494633,\n",
       " 2.0: 0.2150537634408603,\n",
       " 3.0: 0.12903225806451615,\n",
       " 4.0: 0.0752688172043011,\n",
       " 5.0: 0.09677419354838714,\n",
       " 6.0: 0.04301075268817206,\n",
       " 7.0: 0.03225806451612904,\n",
       " 8.0: 0.04301075268817206,\n",
       " 9.0: 0.04301075268817206,\n",
       " 10.0: 0.03225806451612904}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use inferences to check specific cpds\n",
    "ie = InferenceEngine(bn_cancer)\n",
    "#here we check cpds for a patient with the treatment combination: had chemotherapy, did not have a surgery, had Beam radiation.\n",
    "ie.query({'CHEMO': 1, 'NO_SURG': 1, 'RADIATION': 1, 'RAD_SURG_SEQ': 0})['SRV_TIME_YR']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
